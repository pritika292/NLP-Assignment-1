{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and pre-process training dataset\n",
    "\n",
    "def standardize_punctuations(text):\n",
    "    replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "    for symbol, replacement in replacement_rules.items():\n",
    "        text = text.replace(symbol, replacement)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(tokenized_sentence):\n",
    "    filtered_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        if word not in stop_words:\n",
    "            filtered_sentence.append(word)\n",
    "    return filtered_sentence\n",
    "\n",
    "def get_data(input_path) :\n",
    "     with open(input_path) as read_handle:\n",
    "        text = read_handle.read()\n",
    "        text = text.lower()\n",
    "        text = standardize_punctuations(text)\n",
    "        tokenized_sentences = sent_tokenize(text)\n",
    "        return tokenized_sentences\n",
    "     \n",
    "def get_data_split(input_path) :\n",
    "    tokenized_sentences = get_data(input_path)\n",
    "    total_lines = len(tokenized_sentences)\n",
    "    train_set = int(0.85 * total_lines)\n",
    "    return tokenized_sentences[:train_set], tokenized_sentences[train_set:]\n",
    "     \n",
    "def tokenize_text(sentence_data, tokenized_path):\n",
    "    with open(tokenized_path, 'w') as write_handle:\n",
    "        for sentence in sentence_data:\n",
    "            sentence = re.sub(r'[?!:.;,#@-`()]', '', sentence)\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "            tokenized_sentence = remove_stop_words(tokenized_sentence)\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_sentence]\n",
    "            if lemmatized_words:\n",
    "                lemmatized_words = lemmatized_words +['[END]']\n",
    "                write_handle.write(','.join(lemmatized_words))\n",
    "                write_handle.write('\\n')\n",
    "\n",
    "def tokenize_text_unk(sentence_data, tokenized_path, vocab):\n",
    "    with open(tokenized_path, 'w') as write_handle:\n",
    "        for sentence in sentence_data:\n",
    "            sentence = re.sub(r'[?!:.;,#@-`()]', '', sentence)\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "            tokenized_sentence = remove_stop_words(tokenized_sentence)\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_sentence]\n",
    "            tokenized_words = [\"<UNK>\" if word not in vocab else word for word in lemmatized_words]\n",
    "            \n",
    "            if tokenized_words:\n",
    "                tokenized_words = tokenized_words + ['[END]']\n",
    "                write_handle.write(','.join(tokenized_words))\n",
    "                write_handle.write('\\n')\n",
    "\n",
    "def tokenize_with_threshold(sentence_data, tokenized_path, frequencies, threshold):\n",
    "    with open(tokenized_path, 'w') as write_handle:\n",
    "        for sentence in sentence_data:\n",
    "            sentence = re.sub(r'[?!:.;,#@-`()]', '', sentence)\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "            tokenized_sentence = remove_stop_words(tokenized_sentence)\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_sentence]\n",
    "            tokenized_words = [word if frequencies[word] >= threshold else \"<UNK>\" for word in lemmatized_words]\n",
    "            if tokenized_words:\n",
    "                tokenized_words = tokenized_words + ['[END]']\n",
    "                write_handle.write(','.join(tokenized_words))\n",
    "                write_handle.write('\\n')\n",
    "    vocab = set(word for word, freq in frequencies.items() if freq >= threshold)\n",
    "    vocab.add(\"<UNK>\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_counts(data_file_path):\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   total_words = 0\n",
    "   \n",
    "   with open(data_file_path) as read_handle:\n",
    "      lines = read_handle.readlines()\n",
    "      for line in lines:\n",
    "         data = line.split(',')\n",
    "         n = len(data)\n",
    "         total_words += n\n",
    "         for i in range(n):\n",
    "            if i+1 < n:\n",
    "               if (data[i], data[i+1]) in bigramCounts:\n",
    "                  bigramCounts[(data[i], data[i + 1])] += 1\n",
    "               else:\n",
    "                  bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "            if data[i] in unigramCounts:\n",
    "               unigramCounts[data[i]] += 1\n",
    "            else:\n",
    "               unigramCounts[data[i]] = 1\n",
    "   return unigramCounts, bigramCounts, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probability(unigram_counts, bigram_counts, output_file_path):\n",
    "    bigram_prob = {}\n",
    "    list_of_bigrams = bigram_counts.keys()\n",
    "    with open(output_file_path, 'w') as write_handle:\n",
    "        for bigram in list_of_bigrams:\n",
    "            bigram_prob[bigram] = (bigram_counts[bigram])/(unigram_counts[bigram[0]])\n",
    "            write_handle.write(\"P{} = {}\".format(bigram, bigram_prob[bigram]))\n",
    "            write_handle.write(\"\\n\")\n",
    "    return bigram_prob\n",
    "\n",
    "def get_unigram_probability(unigram_counts, total_train_words, output_file_path):\n",
    "    unigram_prob = {}\n",
    "    listOfUnigrams = unigram_counts.keys()\n",
    "    with open(output_file_path, 'w') as write_handle:\n",
    "        for unigram in listOfUnigrams:\n",
    "            unigram_prob[unigram] = (unigram_counts[unigram])/total_train_words\n",
    "            write_handle.write(\"P({}) = {}\".format(unigram, unigram_prob[unigram]))\n",
    "            write_handle.write(\"\\n\")\n",
    "    return unigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity_unigram(unigrams_train, total_train_words, test_file_path, prob_file_path):\n",
    "    total_words = 0\n",
    "    unigramProb = get_unigram_probability(unigrams_train, total_train_words, prob_file_path)\n",
    "    with open(test_file_path) as read_handle:\n",
    "        lines = read_handle.readlines()\n",
    "        sum_log = 0\n",
    "\n",
    "        for line in lines:\n",
    "            words = line.split(',')\n",
    "            total_words += len(words)\n",
    "            for word in words:\n",
    "                pword = 0\n",
    "                if word in unigramProb:\n",
    "                    pword = unigramProb[word]\n",
    "                sum_log += np.log(pword)\n",
    "        \n",
    "        sum_log = sum_log/total_words\n",
    "        print(\"Perplexity for unigram : \", np.exp2(-1*sum_log))\n",
    "\n",
    "def get_perplexity_bigram(bigrams_train, unigrams_train, test_file_path, prob_file_path):\n",
    "    total_words = 0\n",
    "    bigramProb = get_bigram_probability(unigrams_train, bigrams_train, prob_file_path)\n",
    "    \n",
    "    with open(test_file_path) as read_handle:\n",
    "        lines = read_handle.readlines()\n",
    "        sum_log = 0\n",
    "\n",
    "        for line in lines:\n",
    "            words = line.split(',')\n",
    "            total_words += len(words)\n",
    "            for i in range(len(words)-1):\n",
    "                pbigram = 0\n",
    "                if (words[i], words[i+1]) in bigramProb:\n",
    "                    pbigram = bigramProb[(words[i], words[i+1])]\n",
    "                sum_log += np.log(pbigram)\n",
    "        \n",
    "        sum_log = sum_log/total_words\n",
    "        print(\"Perplexity for bigram : \", np.exp2(-1*sum_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for unigram :  inf\n",
      "Perplexity for bigram :  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kq/2mrz63vj31x1y77_846_6xxw0000gn/T/ipykernel_71554/2732229488.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  sum_log += np.log(pword)\n",
      "/var/folders/kq/2mrz63vj31x1y77_846_6xxw0000gn/T/ipykernel_71554/2732229488.py:35: RuntimeWarning: divide by zero encountered in log\n",
      "  sum_log += np.log(pbigram)\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Unsmoothed ngrams\n",
    "\n",
    "input_file_path = os.path.join(os.getcwd(), \"A1_DATASET/train.txt\")\n",
    "tokenized_file_path = os.path.join(os.getcwd(), \"A1_DATASET/tokenized_train.txt\")\n",
    "input_data  = get_data(input_file_path)\n",
    "tokenize_text(input_data, tokenized_file_path)\n",
    "\n",
    "unigrams_train, bigrams_train, total_train_words = get_ngram_counts(tokenized_file_path)\n",
    "\n",
    "validation_file_path = os.path.join(os.getcwd(), \"A1_DATASET/val.txt\")\n",
    "tokenized_val_file_path = os.path.join(os.getcwd(), \"A1_DATASET/tokenized_val.txt\")\n",
    "val_data = get_data(validation_file_path)\n",
    "tokenize_text(val_data, tokenized_val_file_path)\n",
    "\n",
    "unigram_prob_file_path = os.path.join(os.getcwd(), \"A1_DATASET/unsmoothed_unigram_prob.txt\")\n",
    "bigram_prob_file_path = os.path.join(os.getcwd(), \"A1_DATASET/unsmoothed_bigram_prob.txt\")\n",
    "get_perplexity_unigram(unigrams_train, total_train_words, tokenized_val_file_path, unigram_prob_file_path)\n",
    "get_perplexity_bigram(bigrams_train, unigrams_train, tokenized_val_file_path, bigram_prob_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for unigram :  45.470191681100054\n",
      "Perplexity for bigram :  7.521551415738418\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Unknown words by frequency >= method\n",
    "\n",
    "tokenized_unk_file_path = os.path.join(os.getcwd(), \"A1_DATASET/tokenized_unk_train.txt\")\n",
    "tokenized_unk_val_file_path = os.path.join(os.getcwd(), \"A1_DATASET/tokenized_unk_val.txt\")\n",
    "\n",
    "tokenize_text(input_data, tokenized_unk_file_path)\n",
    "unigrams_train,_,_ = get_ngram_counts(tokenized_unk_file_path)\n",
    "vocab_unk = tokenize_with_threshold(input_data, tokenized_unk_file_path, unigrams_train, 5)\n",
    "\n",
    "unigrams_unk_train, bigrams_unk_train, total_words = get_ngram_counts(tokenized_unk_file_path)\n",
    "\n",
    "tokenize_text_unk(input_data, tokenized_unk_val_file_path, vocab_unk)\n",
    "\n",
    "unigram_unk_prob_file_path = os.path.join(os.getcwd(), \"A1_DATASET/unk_unigram_prob.txt\") \n",
    "bigram_unk_prob_file_path = os.path.join(os.getcwd(), \"A1_DATASET/unk_bigram_prob.txt\") \n",
    "get_perplexity_unigram(unigrams_unk_train, total_words, tokenized_unk_val_file_path, unigram_unk_prob_file_path)\n",
    "get_perplexity_bigram(bigrams_unk_train, unigrams_unk_train, tokenized_unk_val_file_path, bigram_unk_prob_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
